{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD for VOC Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from fastai.vision.all import (RandomSplitter, DataLoader,\n",
    "                               DataLoaders, Learner, SaveModelCallback)\n",
    "\n",
    "from models import SSD, generate_map_data, ssd_body_resnet50\n",
    "from data_utils import BoxMatcher, VOCDataset, tensor2boxes, predict_image\n",
    "from training import (loss_func, localization_loss_metric,\n",
    "                      confidence_loss_metric, recall, precision)\n",
    "\n",
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 300\n",
    "MATCH_THRESHOLD = 0.50\n",
    "BATCH_SIZE = 32\n",
    "NUM_ITERATIONS = 120_000\n",
    "\n",
    "# The following values are derived from https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/Detection/SSD/src/utils.py\n",
    "MAP_SIZES = [38, 19, 10, 5, 3, 1]\n",
    "STEPS = [37.5, 18.75, 9.375, 4.6875, 3.0, 1.0]\n",
    "SCALES = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "ASPECT_RATIOS = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = BoxMatcher(MAP_SIZES, STEPS, SCALES, ASPECT_RATIOS, MATCH_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voc_dataset(data_dir: Path, categories: Dict[int, str] = None, ignore_difficult: bool = False) -> Dataset:\n",
    "    \"\"\"Create a dataset object using data in the VOC challenge format\"\"\"\n",
    "    IMG_DIR = data_dir / \"JPEGImages\"\n",
    "    ANN_DIR = data_dir / \"Annotations\"\n",
    "    ann_fns = [ANN_DIR / path for path in os.listdir(ANN_DIR)]\n",
    "    \n",
    "    voc_dataset = VOCDataset(ann_fns, IMG_DIR, IMG_SIZE, matcher, True, categories, ignore_difficult)\n",
    "    \n",
    "    return voc_dataset\n",
    "\n",
    "# Initialize training set with VOC 2012 data and initialise empty validation\n",
    "# set with identical categories\n",
    "train_ds = get_voc_dataset(Path(\"./data/voc2012\"))\n",
    "voc2007 = get_voc_dataset(Path(\"./data/voc2007\"), categories=train_ds.categories)\n",
    "train_ds.data.extend(voc2007.data)\n",
    "\n",
    "# Use 5% of training set for validation\n",
    "valid_ds = copy.deepcopy(train_ds)\n",
    "_, valid_idxs = RandomSplitter(0.05)(valid_ds.data)\n",
    "valid_ds.data = [valid_ds.data[idx] for idx in valid_idxs]\n",
    "\n",
    "train_ds.is_train = True\n",
    "valid_ds.is_train = False\n",
    "\n",
    "# Initialize the test set using VOC 2007\n",
    "test_ds = get_voc_dataset(Path(\"./data/voc2007_test\"), categories=train_ds.categories, ignore_difficult=True)\n",
    "test_ds.is_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dl = DataLoader(train_ds, bs=BATCH_SIZE, num_workers=32, drop_last=False, shuffle=True, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, bs=BATCH_SIZE, num_workers=32, drop_last=False, shuffle=True, pin_memory=True)\n",
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "dls.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Image in Training Dataset\n",
    "img, targ = train_ds[0]\n",
    "targ_boxes, targ_classes, _ = tensor2boxes(matcher.default_boxes, targ)\n",
    "print(f\"Matched {len(targ_boxes)} default boxes in this image.\")\n",
    "train_ds.show_img(img, (targ_boxes, targ_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "**Note:** the following models are available:\n",
    "- Resnet18 (`from models import ssd_body_resnet18`)\n",
    "- Resnet34 (`from models import ssd_body_resnet34`)\n",
    "- Resnet50 (`from models import ssd_body_resnet50`)\n",
    "- Resnet101 (`from models import ssd_body_resnet101`)\n",
    "- Resnet152 (`from models import ssd_body_resnet152`)\n",
    "- MobileNet v2 (`from models import ssd_body_mobilenet_v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = ssd_body_resnet50().cuda()\n",
    "map_data = generate_map_data(IMG_SIZE, MAP_SIZES, body)\n",
    "boxes_per_cell = [2 + 2*len(ratios) for ratios in ASPECT_RATIOS]\n",
    "model = SSD(body, map_data, boxes_per_cell, train_ds.num_classes)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, model,\n",
    "    loss_func=loss_func,\n",
    "    metrics=[localization_loss_metric, confidence_loss_metric, recall, precision],\n",
    "    model_dir='trained_models',\n",
    "    cbs=[SaveModelCallback()],\n",
    "    wd=5e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(300, lr_max=1e-3, div=100, wd=5e-4)\n",
    "learn.save('final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('model')\n",
    "model = learn.model\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, targ = train_ds[0]\n",
    "targ_boxes, targ_classes, _ = tensor2boxes(matcher.default_boxes, targ)\n",
    "pred_boxes, pred_classes, pred_confs = predict_image(model, img.cuda(), matcher.default_boxes.cuda(), conf_threshold = 0.50, iou_threshold = 0.45)\n",
    "valid_ds.show_img(img, ([], []), (pred_boxes, pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_avg_precision = test_ds.calculate_map(model)\n",
    "print(f\"{round(mean_avg_precision * 100, 2)} mAP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
