{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer - French to English Translation\n",
    "\n",
    "This notebook contains an implementation of the Transformer architecture from [Attention Is All You Need (Vaswani et al. 2017)](https://arxiv.org/pdf/1706.03762.pdf). Data was obtained from http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from einops import rearrange\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_PCT = 0.05\n",
    "BATCH_SIZE = 512\n",
    "MAX_TOKENS_PER_SENTENCE = 30\n",
    "DATA_PATH = Path(\"raw-data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Va !</td>\n",
       "      <td>Go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salut !</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Salut.</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cours !</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Courez !</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        src   tgt\n",
       "0      Va !   Go.\n",
       "1   Salut !   Hi.\n",
       "2    Salut.   Hi.\n",
       "3   Cours !  Run!\n",
       "4  Courez !  Run!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=\"\\t\", header=None, names=[\"tgt\", \"src\", \"metadata\"])\n",
    "df = df[[\"src\", \"tgt\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179,887 training examples\n"
     ]
    }
   ],
   "source": [
    "# Remove Long Sentences (to conserve GPU memory during training)\n",
    "df = df[df[\"src\"].apply(lambda txt: len(txt.split()) <= MAX_TOKENS_PER_SENTENCE)]\n",
    "df = df[df[\"tgt\"].apply(lambda txt: len(txt.split()) <= MAX_TOKENS_PER_SENTENCE)]\n",
    "print(f\"Loaded {df.shape[0]:,} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tokenizer\n",
    "rules = [\n",
    "    partial(lowercase, add_bos=True, add_eos=False),\n",
    "    rm_useless_spaces,\n",
    "]\n",
    "\n",
    "src_tok = Tokenizer(WordTokenizer('fr'), rules=rules)\n",
    "tgt_tok = Tokenizer(WordTokenizer('en'), rules=rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Setup Numericalizer\n",
    "src_txts = L(df[\"src\"].values.tolist())\n",
    "tgt_txts = L(df[\"tgt\"].values.tolist())\n",
    "\n",
    "src_toks = src_txts.map(src_tok)\n",
    "tgt_toks = tgt_txts.map(tgt_tok)\n",
    "\n",
    "src_num = Numericalize(max_vocab=30000)\n",
    "tgt_num = Numericalize(max_vocab=30000)\n",
    "\n",
    "src_num.setup(src_toks)\n",
    "tgt_num.setup(tgt_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos une étude a montré que presque dix pour cent des hommes étaient obèses en deux - mille - huit . c' était en augmentation en partant d' environ cinq pour cent en mille - neuf - cent - quatre - vingt .</td>\n",
       "      <td>xxbos a study found that almost 10 % of men were obese in xxunk . that was up from about 5 % in 1980 . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos personne ne t' a demandé d' être d' accord , mais ne peux - tu pas , au moins , accepter qu' il y a des gens qui ont un point de vue différent du tien ? xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos nobody has asked you to agree , but ca n't you at least accept that there are people who hold different views from you ? xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos personne ne vous a demandé d' être d' accord , mais ne pouvez -vous pas , au moins , accepter qu' il y a des gens qui ont un point de vue différent du vôtre ? xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos nobody has asked you to agree , but ca n't you at least accept that there are people who hold different views from you ? xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos qu' est l' âge ? d' abord on oublie les noms , et puis on oublie les visages , puis on oublie de remonter sa braguette , et puis on oublie de la descendre . xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos what is old age ? first you forget names , then you forget faces , then you forget to pull your zipper up , then you forget to pull it down .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos mon petit-déjeuner se compose généralement de café au lait , d' un morceau de pain avec de la confiture , d' une petite banane , d' un morceau d' orange et de prunes xxunk . xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos my breakfast usually consists of coffee with milk , a piece of bread and jam , a small banana , a piece of orange and some dried plums . xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos et la bonne nouvelle est qu' aujourd' hui l' économie se développe à nouveau . salaire , revenus , valeurs xxunk et comptes de xxunk sont tous à la hausse . la pauvreté xxunk . xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos and the good news is that today the economy is growing again . wages , xxunk , home values and retirement accounts are all rising again . poverty is falling again .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos c' était suffisamment grave qu' il ait l' habitude d' arriver en retard au travail , mais qu' il arrive soûl est un comble , et je vais devoir m' en séparer . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos it was bad enough that he usually came to work late , but coming in drunk was the last straw , and i 'm going to have to let him go .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos la prochaine fois que je change d' emploi , j' ai besoin d' un travail qui me permettra de mettre à profit l' expérience que j' ai xxunk jusqu' à maintenant . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos next time i switch jobs , i need work that will let me make use of the experience i 've gained up to now . xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos s' arrêter de fumer , c' est ce qu' il y a de plus facile . je le sais bien pour l' avoir fait moi - même des milliers de fois . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "      <td>xxbos giving up smoking is the easiest thing in the world . i know because i 've done it thousands of times . xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad xxpad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Datasets and Dataloaders\n",
    "src_tfms = [ColReader(\"src\"), src_tok, src_num]\n",
    "tgt_tfms = [ColReader(\"tgt\"), tgt_tok, tgt_num]\n",
    "\n",
    "splits = RandomSplitter(VALID_PCT)(df)\n",
    "dsets = Datasets(df, [src_tfms, tgt_tfms], splits=splits)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    dl_type=SortedDL,\n",
    "    before_batch=partial(pad_input, pad_fields=[0, 1]),\n",
    "    bs=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"An encoder that generates embeddings for each input token. The\n",
    "    embeddings generated are a combination of the tokens' regualar embeddings\n",
    "    and positional embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, vocab_sz: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(vocab_sz, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        exponent = torch.arange(0, embedding_dim, 2).cuda()\n",
    "        exponent = exponent / embedding_dim\n",
    "        period = 10000**exponent\n",
    "        period = period[None, :]\n",
    "        self.register_buffer('period', period)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq_len = x.size(1)\n",
    "        tok_embedding = self.tok_embedding(x) * math.sqrt(self.embedding_dim)\n",
    "        pos_embedding = self.get_pos_embedding(seq_len)\n",
    "        embedding = tok_embedding + pos_embedding\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "    \n",
    "    def get_pos_embedding(self, seq_len: int):\n",
    "        pos = torch.arange(seq_len)[:, None].cuda()\n",
    "        sinusoid_inputs = pos / self.period\n",
    "        sin = sinusoid_inputs.sin()\n",
    "        cos = sinusoid_inputs.cos()\n",
    "        pos_embedding = torch.stack([sin, cos]).view(2, -1).t().reshape(seq_len, -1)\n",
    "        return pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    \"\"\"Position-wise feed forward layer used in both the encoder and the decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention layer, used in both the encoder and decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = embedding_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        \n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, qry: torch.Tensor, key: torch.Tensor, val: torch.Tensor,\n",
    "                mask: torch.Tensor = None):\n",
    "        qry = self.q_proj(qry)\n",
    "        key = self.k_proj(key)\n",
    "        val = self.v_proj(val)\n",
    "        \n",
    "        qry = rearrange(qry, 'b s (h q) -> b h s q', h=self.num_heads)\n",
    "        key = rearrange(key, 'b s (h k) -> b h k s', h=self.num_heads)\n",
    "        val = rearrange(val, 'b s (h v) -> b h s v', h=self.num_heads)\n",
    "        \n",
    "        x = self.attention(qry, key, val, mask)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "    def attention(self, qry: torch.Tensor, key: torch.Tensor, val: torch.Tensor,\n",
    "                  mask: torch.Tensor):\n",
    "        x = qry@key / math.sqrt(self.key_dim)\n",
    "        if mask is not None:\n",
    "            x = x + mask\n",
    "        x = self.attn_softmax(x)\n",
    "        x = x@val\n",
    "        x = rearrange(x, 'b h s v -> b s (h v)')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"A single encoder layer. Many of these layers can be stacked on top of\n",
    "    each other to increase the encoder's depth.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, num_heads: int, hidden_dim: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads, dropout_p)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ff = FeedForwardLayer(embedding_dim, hidden_dim, dropout_p)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.ln1(x + self.mha(x, x, x))\n",
    "        x = self.ln2(x + self.ff(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"A single decoder layer. Many of these layers can be stacked on top of\n",
    "    each other to increase the encoder's depth.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, num_heads: int, hidden_dim: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.masked_mha = MultiHeadAttention(embedding_dim, num_heads, dropout_p)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.mha = MultiHeadAttention(embedding_dim, num_heads, dropout_p)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        self.ff = FeedForwardLayer(embedding_dim, hidden_dim, dropout_p)\n",
    "        self.ln3 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        x, enc_out, mask = inp\n",
    "        x = self.ln1(x + self.masked_mha(x, x, x, mask))\n",
    "        x = self.ln2(x + self.mha(x, enc_out, enc_out))\n",
    "        x = self.ln3(x + self.ff(x))\n",
    "        return x, enc_out, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"The transformer architecture applied to machine translation.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, src_vocab_sz: int, tgt_vocab_sz: int,\n",
    "                 num_layers: int, num_heads: int, hidden_dim: int, dropout_p: float):\n",
    "        super().__init__()\n",
    "        self.src_embeddings = PositionalEncoder(embedding_dim, src_vocab_sz, dropout_p)\n",
    "        self.tgt_embeddings = PositionalEncoder(embedding_dim, tgt_vocab_sz, dropout_p)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*[EncoderLayer(embedding_dim, num_heads, hidden_dim, dropout_p) for _ in range(num_layers)])\n",
    "        self.decoder = nn.Sequential(*[DecoderLayer(embedding_dim, num_heads, hidden_dim, dropout_p) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(embedding_dim, tgt_vocab_sz)\n",
    "        self.fc_out.weight = self.tgt_embeddings.tok_embedding.weight\n",
    "    \n",
    "    def forward(self, enc_in: torch.Tensor, dec_in: torch.Tensor):\n",
    "        enc_in = self.src_embeddings(enc_in)\n",
    "        dec_in = self.tgt_embeddings(dec_in)\n",
    "        \n",
    "        enc_out = self.encoder(enc_in)\n",
    "        \n",
    "        out_seq_len = dec_in.size(1)\n",
    "        dec_mask = torch.triu(torch.full((out_seq_len, out_seq_len), float(\"-Inf\")), 1).cuda()\n",
    "        dec_out, _, _ = self.decoder((dec_in, enc_out, dec_mask))\n",
    "        dec_out = self.fc_out(dec_out)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrepCallback(Callback):\n",
    "    \"\"\"Callback to shift outputs by one so that the model doesn't 'look ahead'\n",
    "    during training. Also, supply the shifted outputs to the model as inputs.\"\"\"\n",
    "    def before_batch(self):\n",
    "        self.learn.xb = (self.x, self.y[:, :-1])\n",
    "        self.learn.yb = (self.y[:, 1:],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncorrectTokenInjectionCallback(Callback):\n",
    "    \"\"\"This callback injects incorrect tokens into the decoder input during\n",
    "    training. By default, transformers implicitly implement teacher forcing during\n",
    "    training, since they are given the (correct) previous tokens in a sentence\n",
    "    when predicting subsequent tokens, which may lead to poorer generalisation.\n",
    "    This callback attempts to remedy this by making the model more robust to\n",
    "    incorrectly-predicted tokens during inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, tgt_vocab_len: int, rate: float = 0.25):\n",
    "        assert 0.0 <= rate <= 1.0\n",
    "        self.tgt_vocab_len = tgt_vocab_len\n",
    "        self.rate = rate\n",
    "    \n",
    "    def before_batch(self):\n",
    "        enc_in, dec_in = self.xb\n",
    "        incorrect_pct = np.round(self.rate * self.epoch / self.n_epoch, 1)\n",
    "        rand_mask = (torch.rand(dec_in.shape) < incorrect_pct).int().cuda()\n",
    "        rand_vals = torch.randint(low=8, high=self.tgt_vocab_len, size=dec_in.shape).cuda()\n",
    "        dec_in = dec_in * (1 - rand_mask) + rand_vals * rand_mask\n",
    "        self.learn.xb = (enc_in, dec_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(inp: torch.Tensor, targ: torch.Tensor, axis: int =-1):\n",
    "    \"\"\"Compute accuracy with `targ` when `pred` is bs * n_classes\n",
    "    while masking out padding tokens\"\"\"\n",
    "    pred,targ = flatten_check(inp.argmax(dim=axis), targ)\n",
    "    return torch.where(targ != 1, pred == targ, False).sum() / (targ != 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and initialize model\n",
    "EMBEDDING_DIM = 512\n",
    "NUM_LAYERS = 6\n",
    "NUM_HEADS = 8\n",
    "HIDDEN_DIM = 2048\n",
    "DROPOUT_P = 0.1\n",
    "\n",
    "src_vocab_sz = len(dls.vocab[0])\n",
    "tgt_vocab_sz = len(dls.vocab[1])\n",
    "\n",
    "model = Transformer(EMBEDDING_DIM, src_vocab_sz, tgt_vocab_sz,\n",
    "                    NUM_LAYERS, NUM_HEADS, HIDDEN_DIM, DROPOUT_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smooth_loss = LabelSmoothingCrossEntropyFlat(eps=0.1, reduction='none')\n",
    "\n",
    "def masked_label_smooth_loss(pred: torch.Tensor, targ: torch.Tensor):\n",
    "    \"\"\"Mask out predictions for padding tokens so that they don't\n",
    "    contribute to the loss\"\"\"\n",
    "    loss = label_smooth_loss(pred, targ)\n",
    "    targ = targ.reshape(-1)\n",
    "    loss = torch.where(targ == 1, torch.zeros(targ.shape).float().cuda(), loss)\n",
    "    return loss.sum() / (targ != 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, model,\n",
    "    loss_func=masked_label_smooth_loss,\n",
    "    cbs=[DataPrepCallback,\n",
    "         IncorrectTokenInjectionCallback(tgt_vocab_sz, 0.25)],\n",
    "    metrics=[masked_accuracy, CorpusBLEUMetric()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.05248074531555176, lr_steep=2.511886486900039e-05)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>masked_accuracy</th>\n",
       "      <th>corpus_bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>32.033127</td>\n",
       "      <td>25.688520</td>\n",
       "      <td>0.146299</td>\n",
       "      <td>0.092956</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.547646</td>\n",
       "      <td>19.835833</td>\n",
       "      <td>0.175566</td>\n",
       "      <td>0.070529</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20.527384</td>\n",
       "      <td>17.768488</td>\n",
       "      <td>0.223209</td>\n",
       "      <td>0.062801</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>18.775843</td>\n",
       "      <td>16.582281</td>\n",
       "      <td>0.260287</td>\n",
       "      <td>0.064996</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17.582838</td>\n",
       "      <td>15.710561</td>\n",
       "      <td>0.289660</td>\n",
       "      <td>0.060551</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.459263</td>\n",
       "      <td>14.837656</td>\n",
       "      <td>0.318942</td>\n",
       "      <td>0.059922</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>15.396769</td>\n",
       "      <td>13.922946</td>\n",
       "      <td>0.335019</td>\n",
       "      <td>0.062684</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.252024</td>\n",
       "      <td>13.015379</td>\n",
       "      <td>0.364307</td>\n",
       "      <td>0.065281</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>13.356359</td>\n",
       "      <td>12.086343</td>\n",
       "      <td>0.370678</td>\n",
       "      <td>0.069447</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>12.276947</td>\n",
       "      <td>11.170087</td>\n",
       "      <td>0.394599</td>\n",
       "      <td>0.073416</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.358560</td>\n",
       "      <td>10.312441</td>\n",
       "      <td>0.386898</td>\n",
       "      <td>0.078204</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10.471231</td>\n",
       "      <td>9.471582</td>\n",
       "      <td>0.406237</td>\n",
       "      <td>0.081753</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>9.627905</td>\n",
       "      <td>8.846776</td>\n",
       "      <td>0.424520</td>\n",
       "      <td>0.077286</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>8.847103</td>\n",
       "      <td>7.911920</td>\n",
       "      <td>0.438659</td>\n",
       "      <td>0.086758</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>8.098508</td>\n",
       "      <td>7.201495</td>\n",
       "      <td>0.469744</td>\n",
       "      <td>0.091538</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.428754</td>\n",
       "      <td>6.616618</td>\n",
       "      <td>0.465304</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>6.891355</td>\n",
       "      <td>6.099170</td>\n",
       "      <td>0.482979</td>\n",
       "      <td>0.100625</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>6.342869</td>\n",
       "      <td>5.700943</td>\n",
       "      <td>0.471302</td>\n",
       "      <td>0.105343</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5.914920</td>\n",
       "      <td>5.271328</td>\n",
       "      <td>0.505068</td>\n",
       "      <td>0.106184</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>5.597504</td>\n",
       "      <td>5.033901</td>\n",
       "      <td>0.505278</td>\n",
       "      <td>0.106302</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.148797</td>\n",
       "      <td>4.648096</td>\n",
       "      <td>0.523186</td>\n",
       "      <td>0.120842</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>5.080179</td>\n",
       "      <td>4.326416</td>\n",
       "      <td>0.544296</td>\n",
       "      <td>0.126524</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.591300</td>\n",
       "      <td>4.138192</td>\n",
       "      <td>0.556824</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>4.391700</td>\n",
       "      <td>3.921710</td>\n",
       "      <td>0.575073</td>\n",
       "      <td>0.139784</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>4.315738</td>\n",
       "      <td>3.865821</td>\n",
       "      <td>0.566881</td>\n",
       "      <td>0.134061</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.000520</td>\n",
       "      <td>3.732601</td>\n",
       "      <td>0.557621</td>\n",
       "      <td>0.144212</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.860537</td>\n",
       "      <td>3.863737</td>\n",
       "      <td>0.559180</td>\n",
       "      <td>0.131114</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.658184</td>\n",
       "      <td>3.346393</td>\n",
       "      <td>0.620922</td>\n",
       "      <td>0.169566</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.491279</td>\n",
       "      <td>3.247234</td>\n",
       "      <td>0.637643</td>\n",
       "      <td>0.176350</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.370669</td>\n",
       "      <td>3.234320</td>\n",
       "      <td>0.632821</td>\n",
       "      <td>0.173292</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.445094</td>\n",
       "      <td>3.101286</td>\n",
       "      <td>0.649096</td>\n",
       "      <td>0.188876</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.127509</td>\n",
       "      <td>3.015409</td>\n",
       "      <td>0.666476</td>\n",
       "      <td>0.199467</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.121685</td>\n",
       "      <td>3.021973</td>\n",
       "      <td>0.663503</td>\n",
       "      <td>0.193501</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.983305</td>\n",
       "      <td>2.921016</td>\n",
       "      <td>0.679667</td>\n",
       "      <td>0.204993</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.877050</td>\n",
       "      <td>2.832585</td>\n",
       "      <td>0.694581</td>\n",
       "      <td>0.215226</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.846512</td>\n",
       "      <td>2.991363</td>\n",
       "      <td>0.690173</td>\n",
       "      <td>0.215491</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.790596</td>\n",
       "      <td>2.740209</td>\n",
       "      <td>0.711455</td>\n",
       "      <td>0.228009</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.711870</td>\n",
       "      <td>2.701180</td>\n",
       "      <td>0.715280</td>\n",
       "      <td>0.235129</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.658999</td>\n",
       "      <td>2.604497</td>\n",
       "      <td>0.733160</td>\n",
       "      <td>0.244561</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.586999</td>\n",
       "      <td>2.662122</td>\n",
       "      <td>0.732330</td>\n",
       "      <td>0.244386</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.558996</td>\n",
       "      <td>2.555402</td>\n",
       "      <td>0.740877</td>\n",
       "      <td>0.250966</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.752291</td>\n",
       "      <td>2.711627</td>\n",
       "      <td>0.694643</td>\n",
       "      <td>0.215161</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.611556</td>\n",
       "      <td>2.735562</td>\n",
       "      <td>0.696560</td>\n",
       "      <td>0.223968</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.620324</td>\n",
       "      <td>2.652939</td>\n",
       "      <td>0.709861</td>\n",
       "      <td>0.228305</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.485280</td>\n",
       "      <td>2.651639</td>\n",
       "      <td>0.711629</td>\n",
       "      <td>0.228629</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.525136</td>\n",
       "      <td>2.596854</td>\n",
       "      <td>0.718439</td>\n",
       "      <td>0.239626</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.392313</td>\n",
       "      <td>2.565801</td>\n",
       "      <td>0.727470</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.451559</td>\n",
       "      <td>2.564862</td>\n",
       "      <td>0.727870</td>\n",
       "      <td>0.240544</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.381745</td>\n",
       "      <td>2.553842</td>\n",
       "      <td>0.723358</td>\n",
       "      <td>0.249584</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.350043</td>\n",
       "      <td>2.546200</td>\n",
       "      <td>0.733640</td>\n",
       "      <td>0.255657</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.254279</td>\n",
       "      <td>2.571514</td>\n",
       "      <td>0.735281</td>\n",
       "      <td>0.254536</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.269230</td>\n",
       "      <td>2.451679</td>\n",
       "      <td>0.750055</td>\n",
       "      <td>0.260915</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.204486</td>\n",
       "      <td>2.483826</td>\n",
       "      <td>0.745879</td>\n",
       "      <td>0.257038</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.153304</td>\n",
       "      <td>2.451831</td>\n",
       "      <td>0.751548</td>\n",
       "      <td>0.265159</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.159900</td>\n",
       "      <td>2.418611</td>\n",
       "      <td>0.757532</td>\n",
       "      <td>0.269313</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.200650</td>\n",
       "      <td>2.440336</td>\n",
       "      <td>0.751178</td>\n",
       "      <td>0.268921</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.078142</td>\n",
       "      <td>2.437560</td>\n",
       "      <td>0.760897</td>\n",
       "      <td>0.275403</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.062864</td>\n",
       "      <td>2.402523</td>\n",
       "      <td>0.765402</td>\n",
       "      <td>0.279111</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.060702</td>\n",
       "      <td>2.374844</td>\n",
       "      <td>0.768489</td>\n",
       "      <td>0.277726</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.974128</td>\n",
       "      <td>2.346451</td>\n",
       "      <td>0.773861</td>\n",
       "      <td>0.283246</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.041040</td>\n",
       "      <td>2.371130</td>\n",
       "      <td>0.771748</td>\n",
       "      <td>0.281624</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.991279</td>\n",
       "      <td>2.332907</td>\n",
       "      <td>0.776404</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.982775</td>\n",
       "      <td>2.399393</td>\n",
       "      <td>0.772401</td>\n",
       "      <td>0.284985</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.965449</td>\n",
       "      <td>2.333363</td>\n",
       "      <td>0.779275</td>\n",
       "      <td>0.286513</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.903986</td>\n",
       "      <td>2.340432</td>\n",
       "      <td>0.782309</td>\n",
       "      <td>0.291147</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.885512</td>\n",
       "      <td>2.338218</td>\n",
       "      <td>0.779407</td>\n",
       "      <td>0.287146</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.885030</td>\n",
       "      <td>2.320725</td>\n",
       "      <td>0.782686</td>\n",
       "      <td>0.294323</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.845772</td>\n",
       "      <td>2.317202</td>\n",
       "      <td>0.785907</td>\n",
       "      <td>0.294307</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.855191</td>\n",
       "      <td>2.296094</td>\n",
       "      <td>0.788022</td>\n",
       "      <td>0.298776</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.833395</td>\n",
       "      <td>2.312597</td>\n",
       "      <td>0.783695</td>\n",
       "      <td>0.294371</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.831243</td>\n",
       "      <td>2.289673</td>\n",
       "      <td>0.787635</td>\n",
       "      <td>0.297779</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.785344</td>\n",
       "      <td>2.280592</td>\n",
       "      <td>0.789629</td>\n",
       "      <td>0.300694</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.783193</td>\n",
       "      <td>2.288414</td>\n",
       "      <td>0.791959</td>\n",
       "      <td>0.300394</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.770976</td>\n",
       "      <td>2.279104</td>\n",
       "      <td>0.792588</td>\n",
       "      <td>0.303087</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.766313</td>\n",
       "      <td>2.281066</td>\n",
       "      <td>0.792501</td>\n",
       "      <td>0.303326</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.764039</td>\n",
       "      <td>2.266816</td>\n",
       "      <td>0.794100</td>\n",
       "      <td>0.303355</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.738762</td>\n",
       "      <td>2.300409</td>\n",
       "      <td>0.791087</td>\n",
       "      <td>0.303884</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.736381</td>\n",
       "      <td>2.280146</td>\n",
       "      <td>0.791338</td>\n",
       "      <td>0.305730</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.729257</td>\n",
       "      <td>2.269578</td>\n",
       "      <td>0.795942</td>\n",
       "      <td>0.307793</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.726753</td>\n",
       "      <td>2.318983</td>\n",
       "      <td>0.788534</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.683755</td>\n",
       "      <td>2.247798</td>\n",
       "      <td>0.797262</td>\n",
       "      <td>0.309561</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.678350</td>\n",
       "      <td>2.249146</td>\n",
       "      <td>0.799295</td>\n",
       "      <td>0.310801</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.704453</td>\n",
       "      <td>2.309818</td>\n",
       "      <td>0.792951</td>\n",
       "      <td>0.308968</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.646387</td>\n",
       "      <td>2.269279</td>\n",
       "      <td>0.797978</td>\n",
       "      <td>0.311309</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.653324</td>\n",
       "      <td>2.255645</td>\n",
       "      <td>0.800579</td>\n",
       "      <td>0.314060</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.640325</td>\n",
       "      <td>2.268014</td>\n",
       "      <td>0.798918</td>\n",
       "      <td>0.311498</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.624142</td>\n",
       "      <td>2.255666</td>\n",
       "      <td>0.801099</td>\n",
       "      <td>0.313812</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.642429</td>\n",
       "      <td>2.249533</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.315844</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.629075</td>\n",
       "      <td>2.270109</td>\n",
       "      <td>0.799872</td>\n",
       "      <td>0.315277</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.628961</td>\n",
       "      <td>2.274138</td>\n",
       "      <td>0.800256</td>\n",
       "      <td>0.311758</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.624850</td>\n",
       "      <td>2.259113</td>\n",
       "      <td>0.801726</td>\n",
       "      <td>0.314925</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.609630</td>\n",
       "      <td>2.266182</td>\n",
       "      <td>0.802210</td>\n",
       "      <td>0.315783</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.604907</td>\n",
       "      <td>2.253230</td>\n",
       "      <td>0.803152</td>\n",
       "      <td>0.314709</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.611414</td>\n",
       "      <td>2.241282</td>\n",
       "      <td>0.805139</td>\n",
       "      <td>0.318198</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.575888</td>\n",
       "      <td>2.234037</td>\n",
       "      <td>0.805336</td>\n",
       "      <td>0.318281</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.566720</td>\n",
       "      <td>2.255537</td>\n",
       "      <td>0.803018</td>\n",
       "      <td>0.318207</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.580667</td>\n",
       "      <td>2.230479</td>\n",
       "      <td>0.805654</td>\n",
       "      <td>0.319683</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.549336</td>\n",
       "      <td>2.236693</td>\n",
       "      <td>0.805051</td>\n",
       "      <td>0.320838</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.556799</td>\n",
       "      <td>2.258588</td>\n",
       "      <td>0.804544</td>\n",
       "      <td>0.320910</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.558082</td>\n",
       "      <td>2.236970</td>\n",
       "      <td>0.806708</td>\n",
       "      <td>0.320244</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.546355</td>\n",
       "      <td>2.239103</td>\n",
       "      <td>0.807513</td>\n",
       "      <td>0.322169</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.557982</td>\n",
       "      <td>2.241898</td>\n",
       "      <td>0.805737</td>\n",
       "      <td>0.321940</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.546976</td>\n",
       "      <td>2.262250</td>\n",
       "      <td>0.801039</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.555818</td>\n",
       "      <td>2.272790</td>\n",
       "      <td>0.802887</td>\n",
       "      <td>0.317629</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.525087</td>\n",
       "      <td>2.236534</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>0.322748</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.506162</td>\n",
       "      <td>2.232274</td>\n",
       "      <td>0.808516</td>\n",
       "      <td>0.323629</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.531680</td>\n",
       "      <td>2.299933</td>\n",
       "      <td>0.794755</td>\n",
       "      <td>0.320297</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.511702</td>\n",
       "      <td>2.227540</td>\n",
       "      <td>0.809490</td>\n",
       "      <td>0.324698</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.509794</td>\n",
       "      <td>2.243264</td>\n",
       "      <td>0.807293</td>\n",
       "      <td>0.323694</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.518661</td>\n",
       "      <td>2.244509</td>\n",
       "      <td>0.807993</td>\n",
       "      <td>0.325229</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.499553</td>\n",
       "      <td>2.231704</td>\n",
       "      <td>0.810333</td>\n",
       "      <td>0.325083</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.493046</td>\n",
       "      <td>2.231620</td>\n",
       "      <td>0.809864</td>\n",
       "      <td>0.325727</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.484363</td>\n",
       "      <td>2.228412</td>\n",
       "      <td>0.810187</td>\n",
       "      <td>0.325698</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.496263</td>\n",
       "      <td>2.222818</td>\n",
       "      <td>0.810704</td>\n",
       "      <td>0.326006</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.502389</td>\n",
       "      <td>2.224745</td>\n",
       "      <td>0.810276</td>\n",
       "      <td>0.326868</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.485505</td>\n",
       "      <td>2.235371</td>\n",
       "      <td>0.810949</td>\n",
       "      <td>0.327765</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.484703</td>\n",
       "      <td>2.222741</td>\n",
       "      <td>0.811205</td>\n",
       "      <td>0.326153</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.471648</td>\n",
       "      <td>2.219214</td>\n",
       "      <td>0.811546</td>\n",
       "      <td>0.327452</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.490001</td>\n",
       "      <td>2.235924</td>\n",
       "      <td>0.810799</td>\n",
       "      <td>0.327920</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.478846</td>\n",
       "      <td>2.253467</td>\n",
       "      <td>0.810229</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.556048</td>\n",
       "      <td>2.296803</td>\n",
       "      <td>0.795467</td>\n",
       "      <td>0.314168</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.533203</td>\n",
       "      <td>2.306681</td>\n",
       "      <td>0.793146</td>\n",
       "      <td>0.315147</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.522396</td>\n",
       "      <td>2.289557</td>\n",
       "      <td>0.793050</td>\n",
       "      <td>0.314451</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.514780</td>\n",
       "      <td>2.295572</td>\n",
       "      <td>0.795794</td>\n",
       "      <td>0.316499</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.525469</td>\n",
       "      <td>2.287920</td>\n",
       "      <td>0.796414</td>\n",
       "      <td>0.315379</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.529027</td>\n",
       "      <td>2.292856</td>\n",
       "      <td>0.795652</td>\n",
       "      <td>0.317051</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.523986</td>\n",
       "      <td>2.285227</td>\n",
       "      <td>0.796794</td>\n",
       "      <td>0.317008</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.516985</td>\n",
       "      <td>2.299309</td>\n",
       "      <td>0.794945</td>\n",
       "      <td>0.317109</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.512860</td>\n",
       "      <td>2.279584</td>\n",
       "      <td>0.797656</td>\n",
       "      <td>0.318590</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.506218</td>\n",
       "      <td>2.273842</td>\n",
       "      <td>0.797233</td>\n",
       "      <td>0.318750</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.498181</td>\n",
       "      <td>2.287424</td>\n",
       "      <td>0.797626</td>\n",
       "      <td>0.319540</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.499416</td>\n",
       "      <td>2.283117</td>\n",
       "      <td>0.799601</td>\n",
       "      <td>0.319792</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.480969</td>\n",
       "      <td>2.282093</td>\n",
       "      <td>0.798346</td>\n",
       "      <td>0.318295</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.492532</td>\n",
       "      <td>2.281033</td>\n",
       "      <td>0.799058</td>\n",
       "      <td>0.321222</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.484185</td>\n",
       "      <td>2.281879</td>\n",
       "      <td>0.798513</td>\n",
       "      <td>0.319522</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.477713</td>\n",
       "      <td>2.271264</td>\n",
       "      <td>0.800614</td>\n",
       "      <td>0.320809</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.489743</td>\n",
       "      <td>2.285584</td>\n",
       "      <td>0.798565</td>\n",
       "      <td>0.319691</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.478680</td>\n",
       "      <td>2.277472</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.321052</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.470084</td>\n",
       "      <td>2.280512</td>\n",
       "      <td>0.799901</td>\n",
       "      <td>0.321955</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.474479</td>\n",
       "      <td>2.277725</td>\n",
       "      <td>0.801223</td>\n",
       "      <td>0.320365</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.483530</td>\n",
       "      <td>2.279322</td>\n",
       "      <td>0.802181</td>\n",
       "      <td>0.323286</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.454217</td>\n",
       "      <td>2.283679</td>\n",
       "      <td>0.799063</td>\n",
       "      <td>0.322396</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.463680</td>\n",
       "      <td>2.264808</td>\n",
       "      <td>0.802658</td>\n",
       "      <td>0.323754</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.463530</td>\n",
       "      <td>2.278281</td>\n",
       "      <td>0.799130</td>\n",
       "      <td>0.323123</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.472065</td>\n",
       "      <td>2.277338</td>\n",
       "      <td>0.800544</td>\n",
       "      <td>0.323341</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.475138</td>\n",
       "      <td>2.273869</td>\n",
       "      <td>0.802050</td>\n",
       "      <td>0.323986</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.454328</td>\n",
       "      <td>2.273995</td>\n",
       "      <td>0.800698</td>\n",
       "      <td>0.322246</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.448525</td>\n",
       "      <td>2.273719</td>\n",
       "      <td>0.801120</td>\n",
       "      <td>0.323234</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.463218</td>\n",
       "      <td>2.281853</td>\n",
       "      <td>0.801090</td>\n",
       "      <td>0.323567</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.438048</td>\n",
       "      <td>2.277115</td>\n",
       "      <td>0.801187</td>\n",
       "      <td>0.323803</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.447599</td>\n",
       "      <td>2.278281</td>\n",
       "      <td>0.802684</td>\n",
       "      <td>0.324040</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.461850</td>\n",
       "      <td>2.279056</td>\n",
       "      <td>0.801744</td>\n",
       "      <td>0.324645</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.444837</td>\n",
       "      <td>2.281282</td>\n",
       "      <td>0.801852</td>\n",
       "      <td>0.323828</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.441388</td>\n",
       "      <td>2.267896</td>\n",
       "      <td>0.802077</td>\n",
       "      <td>0.323635</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.447799</td>\n",
       "      <td>2.273041</td>\n",
       "      <td>0.801824</td>\n",
       "      <td>0.324931</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.437781</td>\n",
       "      <td>2.280679</td>\n",
       "      <td>0.801155</td>\n",
       "      <td>0.324029</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.445161</td>\n",
       "      <td>2.276796</td>\n",
       "      <td>0.802598</td>\n",
       "      <td>0.325013</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.426509</td>\n",
       "      <td>2.271454</td>\n",
       "      <td>0.801423</td>\n",
       "      <td>0.324314</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.442856</td>\n",
       "      <td>2.267136</td>\n",
       "      <td>0.803731</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.442612</td>\n",
       "      <td>2.274866</td>\n",
       "      <td>0.802627</td>\n",
       "      <td>0.325435</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.435112</td>\n",
       "      <td>2.273447</td>\n",
       "      <td>0.803353</td>\n",
       "      <td>0.325955</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.428183</td>\n",
       "      <td>2.278355</td>\n",
       "      <td>0.801837</td>\n",
       "      <td>0.325406</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.423853</td>\n",
       "      <td>2.277765</td>\n",
       "      <td>0.801748</td>\n",
       "      <td>0.325548</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.414908</td>\n",
       "      <td>2.274854</td>\n",
       "      <td>0.803119</td>\n",
       "      <td>0.326430</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.424646</td>\n",
       "      <td>2.271496</td>\n",
       "      <td>0.803691</td>\n",
       "      <td>0.326370</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.440336</td>\n",
       "      <td>2.274733</td>\n",
       "      <td>0.803436</td>\n",
       "      <td>0.327086</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.427950</td>\n",
       "      <td>2.269888</td>\n",
       "      <td>0.803055</td>\n",
       "      <td>0.325852</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.424687</td>\n",
       "      <td>2.285236</td>\n",
       "      <td>0.801783</td>\n",
       "      <td>0.325725</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.425669</td>\n",
       "      <td>2.271051</td>\n",
       "      <td>0.803550</td>\n",
       "      <td>0.326433</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.430277</td>\n",
       "      <td>2.276600</td>\n",
       "      <td>0.803128</td>\n",
       "      <td>0.326443</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.420973</td>\n",
       "      <td>2.269930</td>\n",
       "      <td>0.804287</td>\n",
       "      <td>0.327683</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.425586</td>\n",
       "      <td>2.273687</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.327449</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.418041</td>\n",
       "      <td>2.274966</td>\n",
       "      <td>0.803158</td>\n",
       "      <td>0.326658</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.412678</td>\n",
       "      <td>2.274386</td>\n",
       "      <td>0.803372</td>\n",
       "      <td>0.326959</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.421082</td>\n",
       "      <td>2.279754</td>\n",
       "      <td>0.803149</td>\n",
       "      <td>0.326381</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.414964</td>\n",
       "      <td>2.275781</td>\n",
       "      <td>0.803134</td>\n",
       "      <td>0.327111</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.413484</td>\n",
       "      <td>2.274889</td>\n",
       "      <td>0.803059</td>\n",
       "      <td>0.325844</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.408874</td>\n",
       "      <td>2.276836</td>\n",
       "      <td>0.803395</td>\n",
       "      <td>0.327678</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.409543</td>\n",
       "      <td>2.275626</td>\n",
       "      <td>0.803543</td>\n",
       "      <td>0.327684</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.424994</td>\n",
       "      <td>2.270197</td>\n",
       "      <td>0.803721</td>\n",
       "      <td>0.327450</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.405200</td>\n",
       "      <td>2.274942</td>\n",
       "      <td>0.803197</td>\n",
       "      <td>0.326383</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.414604</td>\n",
       "      <td>2.269816</td>\n",
       "      <td>0.804073</td>\n",
       "      <td>0.327956</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.409373</td>\n",
       "      <td>2.268687</td>\n",
       "      <td>0.804823</td>\n",
       "      <td>0.327735</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.409714</td>\n",
       "      <td>2.271901</td>\n",
       "      <td>0.803248</td>\n",
       "      <td>0.327365</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.412356</td>\n",
       "      <td>2.275611</td>\n",
       "      <td>0.804243</td>\n",
       "      <td>0.327632</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.404175</td>\n",
       "      <td>2.270894</td>\n",
       "      <td>0.804141</td>\n",
       "      <td>0.328219</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.405154</td>\n",
       "      <td>2.269262</td>\n",
       "      <td>0.803672</td>\n",
       "      <td>0.327340</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.413012</td>\n",
       "      <td>2.271120</td>\n",
       "      <td>0.802769</td>\n",
       "      <td>0.326410</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.415478</td>\n",
       "      <td>2.269560</td>\n",
       "      <td>0.804371</td>\n",
       "      <td>0.328886</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.407020</td>\n",
       "      <td>2.272438</td>\n",
       "      <td>0.804297</td>\n",
       "      <td>0.326720</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.407911</td>\n",
       "      <td>2.268616</td>\n",
       "      <td>0.803778</td>\n",
       "      <td>0.327506</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.399813</td>\n",
       "      <td>2.272763</td>\n",
       "      <td>0.803821</td>\n",
       "      <td>0.327228</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.405406</td>\n",
       "      <td>2.271007</td>\n",
       "      <td>0.804218</td>\n",
       "      <td>0.326782</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.415055</td>\n",
       "      <td>2.276320</td>\n",
       "      <td>0.803672</td>\n",
       "      <td>0.327657</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.400957</td>\n",
       "      <td>2.268788</td>\n",
       "      <td>0.805241</td>\n",
       "      <td>0.328826</td>\n",
       "      <td>00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.413500</td>\n",
       "      <td>2.271319</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.328605</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.404751</td>\n",
       "      <td>2.272357</td>\n",
       "      <td>0.804702</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.408911</td>\n",
       "      <td>2.276197</td>\n",
       "      <td>0.804010</td>\n",
       "      <td>0.328037</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.408162</td>\n",
       "      <td>2.272019</td>\n",
       "      <td>0.804905</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>00:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.406619</td>\n",
       "      <td>2.274490</td>\n",
       "      <td>0.802889</td>\n",
       "      <td>0.326827</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(200, lr_max=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Model Translations (Generated Using Beam Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAM_SIZE = 32  # Number of possible translations to consider at each point in the tree\n",
    "NUM_HYPOTHESES = 16  # The maximum number of translations to generate before stopping\n",
    "MAX_LENGTH = MAX_TOKENS_PER_SENTENCE  # The maximum length of any translation\n",
    "NUM_EXAMPLES = 24\n",
    "\n",
    "tgt_vocab = dls.vocab[1]\n",
    "model = learn.model\n",
    "xb, yb = dls.valid.one_batch()\n",
    "xb = xb[:NUM_EXAMPLES]\n",
    "yb = yb[:NUM_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode():\n",
    "    \"\"\"A node to be used in a beam search tree\"\"\"\n",
    "    \n",
    "    def __init__(self, token_idxs: torch.Tensor, sentence_pos: int, norm_cumulative_ll: float):\n",
    "        self.token_idxs = token_idxs\n",
    "        self.sentence_pos = sentence_pos\n",
    "        self.norm_cumulative_ll = norm_cumulative_ll\n",
    "    \n",
    "    def create_child(self, new_token_idx: int, log_likelihood: float):\n",
    "        token_idxs = torch.cat([self.token_idxs, torch.tensor([new_token_idx])])\n",
    "        sentence_pos = self.sentence_pos + 1\n",
    "        norm_cumulative_ll = (self.norm_cumulative_ll * self.sentence_pos + log_likelihood) / sentence_pos\n",
    "        child = BeamSearchNode(token_idxs, sentence_pos, norm_cumulative_ll)\n",
    "        return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2txt(num_tensor, numericalizer, tokenizer):\n",
    "    \"\"\"Convert embedding indices into words\"\"\"\n",
    "    txt = tokenizer.decode(numericalizer.decode(num_tensor))\n",
    "    txt = txt.replace(\" xxpad\", \"\")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='24' class='' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [24/24 00:29<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>norm_ll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>la première fois que vous rencontrez une personne , vous devriez être attentive à la proximité avec laquelle vous vous tenez par rapport à elle .</td>\n",
       "      <td>the first time you meet people , you should be careful about how near you stand to them .</td>\n",
       "      <td>the first time you meet people , you should be careful about how near you stand to them .</td>\n",
       "      <td>-0.0634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conformément aux closes du contrat , vous pouvez prendre trois jours de congé de deuil pour les funérailles de votre oncle , mais un seul pour celles de votre neveu .</td>\n",
       "      <td>according to the contract you may take three days of bereavement leave for your uncle 's funeral , but only one for your nephew 's .</td>\n",
       "      <td>according to the contract you may take three days of bereavement leave for your uncle 's funeral , but only one for your nephew 's funeral .</td>\n",
       "      <td>-0.1337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>je n' ai jamais imaginé un seul instant que je serais toujours en train de faire ce genre de chose à mon âge .</td>\n",
       "      <td>i never for a moment imagined that i would still be doing this kind of thing at my age .</td>\n",
       "      <td>i never for a moment imagined that i 'd be doing this kind of thing at my age .</td>\n",
       "      <td>-0.1529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ne prenez pas la peine de me réveiller à quatre heures du matin . je ne prévois pas d' aller pêcher demain .</td>\n",
       "      <td>do n't bother waking me up at 4:00 a.m. i do n't plan to go fishing tomorrow .</td>\n",
       "      <td>do n't bother waking me up at 4:00 a.m. i do n't plan to go fishing tomorrow . i do n't plan to go fishing tomorrow .</td>\n",
       "      <td>-0.1733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>il n' y a rien de plus embêtant qu' un groupe de jeunes filles qui essayent toutes de parler en même temps .</td>\n",
       "      <td>there 's nothing more annoying than a group of young girls all trying to talk at the same time .</td>\n",
       "      <td>there 's nothing more annoying than a group of young girls all trying to talk at the same time .</td>\n",
       "      <td>-0.1912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elle lui a recommandé de prendre de longues vacances , il a donc quitté immédiatement le travail et est parti en voyage autour du monde .</td>\n",
       "      <td>she advised him to take a long holiday , so he immediately quit work and took a trip around the world .</td>\n",
       "      <td>she advised him to take a long holiday , so he left work and took a trip around the world .</td>\n",
       "      <td>-0.2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ce qu' est l' âge ? d' abord on oublie les noms , et puis on oublie les visages , puis on oublie de remonter sa braguette , et puis on oublie de la descendre .</td>\n",
       "      <td>what is old age ? first you forget names , then you forget faces , then you forget to pull your zipper up , then you forget to pull it down .</td>\n",
       "      <td>what is old age ? first you forget to pull your age , then you forget to pull it down , then you forget to pull it down .</td>\n",
       "      <td>-0.2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>si j' en parle à ma mère , elle va se faire du souci , donc je ne pense pas que je lui en parlerai .</td>\n",
       "      <td>if i tell my mother , she 'll worry , so i do n't think i 'll tell her .</td>\n",
       "      <td>she is talking to me , so i think about it , so i do n't think about it .</td>\n",
       "      <td>-0.3486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>«   pourquoi vas - tu au japon   ?   » «   pour participer à une conférence à tokyo .   »</td>\n",
       "      <td>why are you going to japan ? \" to attend a conference in tokyo . \"</td>\n",
       "      <td>why are you going to come to japan ? \" in tokyo with a conference .</td>\n",
       "      <td>-0.4561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plus l' on passe de temps à parler une langue étrangère , plus l' on s' améliore à devenir ce qu' un xxunk essaie de dire dans sa propre langue .</td>\n",
       "      <td>the more time you spend speaking a foreign language , the better you get at guessing what non - native speakers are trying to say in your own language .</td>\n",
       "      <td>in what is trying to become a foreign language , you try to become more than one language you get to become a foreign language .</td>\n",
       "      <td>-0.5408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dans ce cours , nous xxunk du temps à vous aider à vous exprimer en ayant plus l' air d' un locuteur natif .</td>\n",
       "      <td>in this course , we 'll spend time helping you sound more like a native speaker .</td>\n",
       "      <td>in a native speaker , we 'll help you sound more a native speaker .</td>\n",
       "      <td>-0.5738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dans le même xxunk de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi - même .</td>\n",
       "      <td>in the same amount of time it would take me to correct all the mistakes in your report , i could write a better report myself .</td>\n",
       "      <td>i could fix all the best report in your report than i could write a better than the same report by myself .</td>\n",
       "      <td>-0.6232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>« est -ce que vous détestez tom ? » « je ne le déteste pas , mais je trouve simplement qu' il est un peu bizarre . »</td>\n",
       "      <td>do you hate tom ? \" i do n't hate him , but i do think he 's a bit strange . \"</td>\n",
       "      <td>i hate you ? \" i just hate tom .</td>\n",
       "      <td>-0.6704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ce n' était que quand j' ai moi - même eu un bébé que j' ai su ce que l' amour maternel représente .</td>\n",
       "      <td>it was not until i had a baby myself that i knew what mother 's love is .</td>\n",
       "      <td>it was not what i felt a baby when i had it was a baby to me .</td>\n",
       "      <td>-0.6763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>les bons amis sont comme les étoiles : on ne les voit pas toujours , mais on sait qu' ils sont toujours là .</td>\n",
       "      <td>good friends are like stars . you do n't always see them , but you know they are always there .</td>\n",
       "      <td>you are good friends , but they still do n't know they are good .</td>\n",
       "      <td>-0.7971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>le rire , c' est comme un essuie - glace . il n' arrête pas la pluie , mais il permet d' avancer .</td>\n",
       "      <td>laughter is like a windshield xxunk . it ca n't stop the rain , but it lets you move ahead .</td>\n",
       "      <td>rain is xxunk the ice cream , but he does n't let him go on . he bring it a ice cream .</td>\n",
       "      <td>-0.8073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l' enquête conclut qu' en dépit de ses xxunk , le directeur général aurait eu connaissance des pratiques illégales , en cours dans l' entreprise .</td>\n",
       "      <td>the xxunk concluded that , despite his xxunk , the chief executive would have had to have known about the illegal practices xxunk in the company .</td>\n",
       "      <td>in spite of the company , the xxunk xxunk xxunk that the investigation would have been in xxunk .</td>\n",
       "      <td>-0.8138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>beaucoup de gens qui ont jusqu' à présent dépensé de l' argent à passer du bon temps doivent être plus prudents avec leur argent .</td>\n",
       "      <td>a lot of people who have up until now been spending money having a good time now need to be more careful with their money .</td>\n",
       "      <td>a lot of people have spent spent spent more money with spent spent spent spent spent .</td>\n",
       "      <td>-0.8211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>je pense que tout le café que j' ai bu dans l' après-midi est la raison pour laquelle je ne dors pas bien .</td>\n",
       "      <td>i think all the coffee that i 've been drinking in the afternoon is why i have n't been sleeping well .</td>\n",
       "      <td>i think the coffee is so i did n't drink why i drank coffee .</td>\n",
       "      <td>-0.9226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>des pirates informatiques ont été en mesure de s' introduire dans le système informatique de l' entreprise et de xxunk la sécurité de son réseau .</td>\n",
       "      <td>hackers were able to break into the company 's computer system and xxunk its xxunk security .</td>\n",
       "      <td>the company was able to xxunk the safety xxunk of the computer company and the system xxunk of the company .</td>\n",
       "      <td>-0.9912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>« entre seulement et dis au patron que tu veux une augmentation . » « plus facile à dire qu' à faire . »</td>\n",
       "      <td>just go in and tell the boss you want a raise . \" that 's easier said than done . \"</td>\n",
       "      <td>say you mean to a boss ? \" only a easier to . \" just want to do .</td>\n",
       "      <td>-1.0741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>«   mes enfants sont déjà adultes . et les vôtres   ?   » «   les miens aussi .   »</td>\n",
       "      <td>my children are already adults . what about yours ? \" mine are too . \"</td>\n",
       "      <td>my children are adults ? \" so are mine , here . \" they are mine .</td>\n",
       "      <td>-1.1716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus de 100 tonnes de poussière et de xxunk de la taille d' un grain de sable xxunk chaque jour la terre depuis l' espace , la plupart d' xxunk passant xxunk .</td>\n",
       "      <td>over 100 tons of dust and sand - sized xxunk xxunk the earth every day from space , most of which goes xxunk .</td>\n",
       "      <td>most xxunk of the space xxunk , the xxunk xxunk xxunk the sand xxunk of the sand xxunk .</td>\n",
       "      <td>-1.2317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>les plus grands chanteurs du monde et la plupart de ses musiciens xxunk , ont été gros ou tout au moins vraiment xxunk .</td>\n",
       "      <td>the world 's greatest singers and most of its famous musicians have been fat or at least xxunk xxunk .</td>\n",
       "      <td>most big everyone , the fatter xxunk , the fatter xxunk at least two or everyone in the big summer .</td>\n",
       "      <td>-1.3978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                    source  \\\n",
       "7                        la première fois que vous rencontrez une personne , vous devriez être attentive à la proximité avec laquelle vous vous tenez par rapport à elle .   \n",
       "2   conformément aux closes du contrat , vous pouvez prendre trois jours de congé de deuil pour les funérailles de votre oncle , mais un seul pour celles de votre neveu .   \n",
       "19                                                          je n' ai jamais imaginé un seul instant que je serais toujours en train de faire ce genre de chose à mon âge .   \n",
       "21                                                            ne prenez pas la peine de me réveiller à quatre heures du matin . je ne prévois pas d' aller pêcher demain .   \n",
       "23                                                            il n' y a rien de plus embêtant qu' un groupe de jeunes filles qui essayent toutes de parler en même temps .   \n",
       "9                                elle lui a recommandé de prendre de longues vacances , il a donc quitté immédiatement le travail et est parti en voyage autour du monde .   \n",
       "0          ce qu' est l' âge ? d' abord on oublie les noms , et puis on oublie les visages , puis on oublie de remonter sa braguette , et puis on oublie de la descendre .   \n",
       "6                                                                     si j' en parle à ma mère , elle va se faire du souci , donc je ne pense pas que je lui en parlerai .   \n",
       "15                                                                               «   pourquoi vas - tu au japon   ?   » «   pour participer à une conférence à tokyo .   »   \n",
       "3                        plus l' on passe de temps à parler une langue étrangère , plus l' on s' améliore à devenir ce qu' un xxunk essaie de dire dans sa propre langue .   \n",
       "16                                                            dans ce cours , nous xxunk du temps à vous aider à vous exprimer en ayant plus l' air d' un locuteur natif .   \n",
       "4                    dans le même xxunk de temps que ça me prendrait de corriger toutes les erreurs de votre rapport , je pourrais écrire un meilleur rapport moi - même .   \n",
       "5                                                     « est -ce que vous détestez tom ? » « je ne le déteste pas , mais je trouve simplement qu' il est un peu bizarre . »   \n",
       "20                                                                    ce n' était que quand j' ai moi - même eu un bébé que j' ai su ce que l' amour maternel représente .   \n",
       "12                                                            les bons amis sont comme les étoiles : on ne les voit pas toujours , mais on sait qu' ils sont toujours là .   \n",
       "17                                                                      le rire , c' est comme un essuie - glace . il n' arrête pas la pluie , mais il permet d' avancer .   \n",
       "8                       l' enquête conclut qu' en dépit de ses xxunk , le directeur général aurait eu connaissance des pratiques illégales , en cours dans l' entreprise .   \n",
       "11                                      beaucoup de gens qui ont jusqu' à présent dépensé de l' argent à passer du bon temps doivent être plus prudents avec leur argent .   \n",
       "18                                                             je pense que tout le café que j' ai bu dans l' après-midi est la raison pour laquelle je ne dors pas bien .   \n",
       "10                      des pirates informatiques ont été en mesure de s' introduire dans le système informatique de l' entreprise et de xxunk la sécurité de son réseau .   \n",
       "13                                                                « entre seulement et dis au patron que tu veux une augmentation . » « plus facile à dire qu' à faire . »   \n",
       "22                                                                                     «   mes enfants sont déjà adultes . et les vôtres   ?   » «   les miens aussi .   »   \n",
       "1          plus de 100 tonnes de poussière et de xxunk de la taille d' un grain de sable xxunk chaque jour la terre depuis l' espace , la plupart d' xxunk passant xxunk .   \n",
       "14                                                les plus grands chanteurs du monde et la plupart de ses musiciens xxunk , ont été gros ou tout au moins vraiment xxunk .   \n",
       "\n",
       "                                                                                                                                                      target  \\\n",
       "7                                                                  the first time you meet people , you should be careful about how near you stand to them .   \n",
       "2                       according to the contract you may take three days of bereavement leave for your uncle 's funeral , but only one for your nephew 's .   \n",
       "19                                                                  i never for a moment imagined that i would still be doing this kind of thing at my age .   \n",
       "21                                                                            do n't bother waking me up at 4:00 a.m. i do n't plan to go fishing tomorrow .   \n",
       "23                                                          there 's nothing more annoying than a group of young girls all trying to talk at the same time .   \n",
       "9                                                    she advised him to take a long holiday , so he immediately quit work and took a trip around the world .   \n",
       "0              what is old age ? first you forget names , then you forget faces , then you forget to pull your zipper up , then you forget to pull it down .   \n",
       "6                                                                                   if i tell my mother , she 'll worry , so i do n't think i 'll tell her .   \n",
       "15                                                                                        why are you going to japan ? \" to attend a conference in tokyo . \"   \n",
       "3   the more time you spend speaking a foreign language , the better you get at guessing what non - native speakers are trying to say in your own language .   \n",
       "16                                                                         in this course , we 'll spend time helping you sound more like a native speaker .   \n",
       "4                            in the same amount of time it would take me to correct all the mistakes in your report , i could write a better report myself .   \n",
       "5                                                                             do you hate tom ? \" i do n't hate him , but i do think he 's a bit strange . \"   \n",
       "20                                                                                 it was not until i had a baby myself that i knew what mother 's love is .   \n",
       "12                                                           good friends are like stars . you do n't always see them , but you know they are always there .   \n",
       "17                                                              laughter is like a windshield xxunk . it ca n't stop the rain , but it lets you move ahead .   \n",
       "8         the xxunk concluded that , despite his xxunk , the chief executive would have had to have known about the illegal practices xxunk in the company .   \n",
       "11                               a lot of people who have up until now been spending money having a good time now need to be more careful with their money .   \n",
       "18                                                   i think all the coffee that i 've been drinking in the afternoon is why i have n't been sleeping well .   \n",
       "10                                                             hackers were able to break into the company 's computer system and xxunk its xxunk security .   \n",
       "13                                                                       just go in and tell the boss you want a raise . \" that 's easier said than done . \"   \n",
       "22                                                                                    my children are already adults . what about yours ? \" mine are too . \"   \n",
       "1                                             over 100 tons of dust and sand - sized xxunk xxunk the earth every day from space , most of which goes xxunk .   \n",
       "14                                                    the world 's greatest singers and most of its famous musicians have been fat or at least xxunk xxunk .   \n",
       "\n",
       "                                                                                                                                      prediction  \\\n",
       "7                                                      the first time you meet people , you should be careful about how near you stand to them .   \n",
       "2   according to the contract you may take three days of bereavement leave for your uncle 's funeral , but only one for your nephew 's funeral .   \n",
       "19                                                               i never for a moment imagined that i 'd be doing this kind of thing at my age .   \n",
       "21                         do n't bother waking me up at 4:00 a.m. i do n't plan to go fishing tomorrow . i do n't plan to go fishing tomorrow .   \n",
       "23                                              there 's nothing more annoying than a group of young girls all trying to talk at the same time .   \n",
       "9                                                    she advised him to take a long holiday , so he left work and took a trip around the world .   \n",
       "0                      what is old age ? first you forget to pull your age , then you forget to pull it down , then you forget to pull it down .   \n",
       "6                                                                      she is talking to me , so i think about it , so i do n't think about it .   \n",
       "15                                                                           why are you going to come to japan ? \" in tokyo with a conference .   \n",
       "3               in what is trying to become a foreign language , you try to become more than one language you get to become a foreign language .   \n",
       "16                                                                           in a native speaker , we 'll help you sound more a native speaker .   \n",
       "4                                    i could fix all the best report in your report than i could write a better than the same report by myself .   \n",
       "5                                                                                                               i hate you ? \" i just hate tom .   \n",
       "20                                                                                it was not what i felt a baby when i had it was a baby to me .   \n",
       "12                                                                             you are good friends , but they still do n't know they are good .   \n",
       "17                                                       rain is xxunk the ice cream , but he does n't let him go on . he bring it a ice cream .   \n",
       "8                                              in spite of the company , the xxunk xxunk xxunk that the investigation would have been in xxunk .   \n",
       "11                                                        a lot of people have spent spent spent more money with spent spent spent spent spent .   \n",
       "18                                                                                 i think the coffee is so i did n't drink why i drank coffee .   \n",
       "10                                  the company was able to xxunk the safety xxunk of the computer company and the system xxunk of the company .   \n",
       "13                                                                             say you mean to a boss ? \" only a easier to . \" just want to do .   \n",
       "22                                                                             my children are adults ? \" so are mine , here . \" they are mine .   \n",
       "1                                                       most xxunk of the space xxunk , the xxunk xxunk xxunk the sand xxunk of the sand xxunk .   \n",
       "14                                          most big everyone , the fatter xxunk , the fatter xxunk at least two or everyone in the big summer .   \n",
       "\n",
       "    norm_ll  \n",
       "7   -0.0634  \n",
       "2   -0.1337  \n",
       "19  -0.1529  \n",
       "21  -0.1733  \n",
       "23  -0.1912  \n",
       "9   -0.2086  \n",
       "0   -0.2311  \n",
       "6   -0.3486  \n",
       "15  -0.4561  \n",
       "3   -0.5408  \n",
       "16  -0.5738  \n",
       "4   -0.6232  \n",
       "5   -0.6704  \n",
       "20  -0.6763  \n",
       "12  -0.7971  \n",
       "17  -0.8073  \n",
       "8   -0.8138  \n",
       "11  -0.8211  \n",
       "18  -0.9226  \n",
       "10  -0.9912  \n",
       "13  -1.0741  \n",
       "22  -1.1716  \n",
       "1   -1.2317  \n",
       "14  -1.3978  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Beam Search\n",
    "period_idx = tgt_vocab.index('.')\n",
    "\n",
    "data = []\n",
    "for src, tgt in progress_bar(list(zip(xb, yb))):\n",
    "    finished: List[BeamSearchNode] = []\n",
    "    in_progress: List[BeamSearchNode] = []\n",
    "    \n",
    "    src_period_count = src.detach().cpu().numpy().tolist().count(period_idx)\n",
    "    root_idxs = torch.tensor([tgt_vocab.index(\"xxbos\")])\n",
    "    root = BeamSearchNode(token_idxs=root_idxs, sentence_pos=0, norm_cumulative_ll=0.0)\n",
    "    in_progress.append(root)\n",
    "    \n",
    "    while len(finished) < NUM_HYPOTHESES:\n",
    "        # Take the node with the highest log-likelihood and generate its child nodes\n",
    "        in_progress.sort(key=lambda node: node.norm_cumulative_ll, reverse=True)\n",
    "        node = in_progress.pop(0)\n",
    "\n",
    "        enc_in = src[None]\n",
    "        dec_in = node.token_idxs[None].to(dls.device)\n",
    "\n",
    "        out = model(enc_in, dec_in)[0][-1].softmax(0)\n",
    "        token_idxs = out.argsort(descending=True)[:BEAM_SIZE]\n",
    "        token_idxs = token_idxs.cpu().numpy().tolist()\n",
    "\n",
    "        for token_idx in token_idxs:\n",
    "            log_likelihood = out[token_idx].log()\n",
    "            child = node.create_child(token_idx, log_likelihood)\n",
    "            \n",
    "            is_max_len = child.sentence_pos >= MAX_LENGTH\n",
    "            is_eos_tok = tgt_vocab[token_idx] in [\"xxeos\"]\n",
    "            pred_period_count = child.token_idxs.numpy().tolist().count(period_idx)\n",
    "            same_period_count = (pred_period_count == src_period_count)\n",
    "            if is_max_len or is_eos_tok or same_period_count:\n",
    "                finished.append(child)\n",
    "            else:\n",
    "                in_progress.append(child)\n",
    "\n",
    "    finished.sort(key=lambda node: node.norm_cumulative_ll, reverse=True)\n",
    "    best_translation = finished[0]\n",
    "    pred_words = num2txt(best_translation.token_idxs[1:], tgt_num, tgt_tok)\n",
    "    norm_ll = round(best_translation.norm_cumulative_ll.item(), 4)\n",
    "\n",
    "    src_words = num2txt(src[1:], src_num, src_tok)\n",
    "    tgt_words = num2txt(tgt[1:], tgt_num, tgt_tok)\n",
    "\n",
    "    data.append({\n",
    "        \"source\": src_words,\n",
    "        \"target\": tgt_words,\n",
    "        \"prediction\": pred_words,\n",
    "        \"norm_ll\": norm_ll,\n",
    "    })\n",
    "\n",
    "valid_df = pd.DataFrame(data).sort_values(\"norm_ll\", ascending=False)\n",
    "valid_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
